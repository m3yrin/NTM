{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NTM-jp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m3yrin/NTM/blob/master/NTM_jp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuOYj2EStn0e",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch implementation of Gaussian Softmax Neural Topic Model\n",
        "auther : @m3yrin\n",
        "\n",
        "### Reference code\n",
        "* yuewang-cuhk/TAKG\n",
        "    * https://github.com/yuewang-cuhk/TAKG\n",
        "* ysmiao/nvdm\n",
        "    * https://github.com/ysmiao/nvdm/\n",
        "* http://tdual.hatenablog.com/entry/2018/04/09/133000\n",
        "\n",
        "### memo\n",
        "* yuewang-cuhk' s NTM implementation is partially used.\n",
        "* tdual' s script is massively cited.\n",
        "* janome tokenizer is used instead of Mecab.\n",
        "\n",
        "***GPU instance is recommended.***\n",
        "If training is too slow, please check instance type.\n",
        "\n",
        "### Dataset\n",
        "livedoor ニュースコーパス / livedoor News Corpus  \n",
        "https://www.rondhuit.com/download.html#ldcc  \n",
        "CC BY-ND 2.1 JP  \n",
        "https://creativecommons.org/licenses/by-nd/2.1/jp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGZlsEd-lRAt",
        "colab_type": "text"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8olctjmZ7l92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install janome\n",
        "\n",
        "import os\n",
        "if not os.path.exists('text'):\n",
        "    !wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "    !tar xvzf ldcc-20140209.tar.gz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KauGq0HK7q8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import random\n",
        "import json\n",
        "import time\n",
        "\n",
        "from urllib import request \n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "\n",
        "import janome\n",
        "from janome import analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wod9NNLml51",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iATs23db7v9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://ohke.hateblo.jp/entry/2017/11/02/230000\n",
        "class NumericReplaceFilter(TokenFilter):\n",
        "    def apply(self, tokens):\n",
        "        for token in tokens:\n",
        "            parts = token.part_of_speech.split(',')\n",
        "            if (parts[0] == '名詞' and parts[1] == '数'):\n",
        "                token.surface = '0'\n",
        "                token.base_form = '0'\n",
        "                token.reading = 'ゼロ'\n",
        "                token.phonetic = 'ゼロ'\n",
        "            yield token\n",
        "\n",
        "            \n",
        "class docTokenizer:\n",
        "    def __init__(self, stopwords, parser=None, include_pos=None, exclude_posdetail=None, exclude_reg=None):\n",
        "    \n",
        "        self.stopwords = stopwords\n",
        "        self.include_pos = include_pos if include_pos else  [\"名詞\", \"動詞\", \"形容詞\"]\n",
        "        self.exclude_posdetail = exclude_posdetail if exclude_posdetail else [\"接尾\", \"数\"]\n",
        "        self.exclude_reg = exclude_reg if exclude_reg else r\"$^\"  # no matching reg\n",
        "        \n",
        "        self.char_filters = [\n",
        "                        UnicodeNormalizeCharFilter(), \n",
        "                        RegexReplaceCharFilter(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", u''), #url\n",
        "                        RegexReplaceCharFilter(r\"\\\"?([-a-zA-Z0-9.`?{}]+\\.jp)\\\"?\", u''), #*.jp\n",
        "                        RegexReplaceCharFilter(self.exclude_reg, u'')\n",
        "                       ]\n",
        "        \n",
        "        self.token_filters = [\n",
        "                         NumericReplaceFilter(),\n",
        "                         POSKeepFilter(self.include_pos),\n",
        "                         POSStopFilter(self.exclude_posdetail), \n",
        "                         LowerCaseFilter()\n",
        "                        ]\n",
        "        \n",
        "        self.analyzer = analyzer.Analyzer(self.char_filters, Tokenizer(), self.token_filters)\n",
        "        \n",
        "    def tokenize(self, text):\n",
        "\n",
        "        tokens = self.analyzer.analyze(text)\n",
        "        tokens = [re.sub(r\",\" ,\"\\t\", str(i)) for i in tokens]\n",
        "        l = [line.split(\"\\t\") for line in tokens]\n",
        "        \n",
        "        #Janome response\n",
        "        #i[] : ['認め', '動詞', '自立', '*', '*', '一段', '連用形', '認める', 'ミトメ', 'ミトメ']\n",
        "\n",
        "        res = []\n",
        "        for i in l:\n",
        "            if i[7] not in self.stopwords:\n",
        "                res.append(i[7])\n",
        "                \n",
        "        return res\n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eJeOhcX8bcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _build_bow_vocab(data, bow_vocab_size, stopwords=None):\n",
        "\n",
        "    bow_dictionary = gensim.corpora.Dictionary(data)\n",
        "    \n",
        "    # Remove STOPWORDS\n",
        "    STOPWORDS = gensim.parsing.preprocessing.STOPWORDS\n",
        "    if stopwords is not None:\n",
        "        STOPWORDS = set(STOPWORDS).union(set(stopwords))\n",
        "    bow_dictionary.filter_tokens(list(map(bow_dictionary.token2id.get, STOPWORDS)))\n",
        "    \n",
        "    # Re-id\n",
        "    bow_dictionary.filter_extremes(no_below=5, no_above=0.2, keep_n=bow_vocab_size)\n",
        "    bow_dictionary.compactify()\n",
        "    bow_dictionary.id2token = dict([(id, t) for t, id in bow_dictionary.token2id.items()])\n",
        "    \n",
        "    print(\"BOW dict length : %d\" % len(bow_dictionary))\n",
        "    \n",
        "    return bow_dictionary\n",
        "\n",
        "def build_bow_vocab(data, stopwords=None):\n",
        "\n",
        "    bow_dictionary = gensim.corpora.Dictionary(data)\n",
        "    \n",
        "    # Remove STOPWORDS\n",
        "    STOPWORDS = gensim.parsing.preprocessing.STOPWORDS\n",
        "    if stopwords is not None:\n",
        "        STOPWORDS = set(STOPWORDS).union(set(stopwords))\n",
        "    bow_dictionary.filter_tokens(list(map(bow_dictionary.token2id.get, STOPWORDS)))\n",
        "    \n",
        "    # Re-id\n",
        "    bow_dictionary.filter_extremes(no_below=5, no_above=0.2)\n",
        "    bow_dictionary.compactify()\n",
        "    bow_dictionary.id2token = dict([(id, t) for t, id in bow_dictionary.token2id.items()])\n",
        "    \n",
        "    print(\"BOW dict length : %d\" % len(bow_dictionary))\n",
        "    \n",
        "    return bow_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PZZIgQFld9A",
        "colab_type": "text"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOdRl9hN8dOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader(object):\n",
        "\n",
        "    def __init__(self, data, bow_vocab, batch_size, shuffle=True):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.bow_vocab = bow_vocab\n",
        "        \n",
        "        self.index = 0\n",
        "        self.pointer = np.array(range(len(data)))\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.bow_data = np.array([bow_vocab.doc2bow(s) for s in data])\n",
        "        \n",
        "        # counting total word number\n",
        "        word_count = []\n",
        "        for bow in self.bow_data:\n",
        "            wc = 0\n",
        "            for (i, c) in bow:\n",
        "                wc += c\n",
        "            word_count.append(wc)\n",
        "        \n",
        "        self.word_count = sum(word_count)\n",
        "        self.data_size = len(data)\n",
        "        \n",
        "        self.shuffle = shuffle\n",
        "        self.reset()\n",
        "\n",
        "    \n",
        "    def reset(self):\n",
        "        \n",
        "        if self.shuffle:\n",
        "            self.pointer = shuffle(self.pointer)\n",
        "        \n",
        "        self.index = 0 \n",
        "    \n",
        "    \n",
        "    # transform bow data into (1 x V) size vector.\n",
        "    def _pad(self, batch):\n",
        "        bow_vocab = len(self.bow_vocab)\n",
        "        res_src_bow = np.zeros((len(batch), bow_vocab))\n",
        "        \n",
        "        for idx, bow in enumerate(batch):\n",
        "            bow_k = [k for k, v in bow]\n",
        "            bow_v = [v for k, v in bow]\n",
        "            res_src_bow[idx, bow_k] = bow_v\n",
        "            \n",
        "        return res_src_bow\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \n",
        "        if self.index >= self.data_size:\n",
        "            self.reset()\n",
        "            raise StopIteration()\n",
        "            \n",
        "        ids = self.pointer[self.index: self.index + self.batch_size]\n",
        "        batch = self.bow_data[ids]\n",
        "        padded = self._pad(batch)\n",
        "        tensor = torch.tensor(padded, dtype=torch.float, device=device)\n",
        "        \n",
        "        self.index += self.batch_size\n",
        "\n",
        "        return tensor\n",
        "    \n",
        "    # for NTM.lasy_predict()\n",
        "    def bow_and_text(self):\n",
        "        if self.index >= self.data_size:\n",
        "            self.reset()\n",
        "            \n",
        "        text = self.data[self.index: self.index + self.batch_size]\n",
        "        batch = self.bow_data[self.index: self.index + self.batch_size]\n",
        "        padded = self._pad(batch)\n",
        "        tensor = torch.tensor(padded, dtype=torch.float, device=device)\n",
        "        self.reset()\n",
        "\n",
        "        return tensor, text\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4vxhGYbmiOB",
        "colab_type": "text"
      },
      "source": [
        "### NTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLWFNinQ8e3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cited : https://github.com/yuewang-cuhk/TAKG/blob/master/pykp/model.py\n",
        "\n",
        "class NTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, topic_num,  l1_strength=0.001):\n",
        "        super(NTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.topic_num = topic_num\n",
        "        self.fc11 = nn.Linear(self.input_dim, hidden_dim)\n",
        "        self.fc12 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc21 = nn.Linear(hidden_dim, topic_num)\n",
        "        self.fc22 = nn.Linear(hidden_dim, topic_num)\n",
        "        self.fcs = nn.Linear(self.input_dim, hidden_dim, bias=False)\n",
        "        self.fcg1 = nn.Linear(topic_num, topic_num)\n",
        "        self.fcg2 = nn.Linear(topic_num, topic_num)\n",
        "        self.fcg3 = nn.Linear(topic_num, topic_num)\n",
        "        self.fcg4 = nn.Linear(topic_num, topic_num)\n",
        "        self.fcd1 = nn.Linear(topic_num, self.input_dim)\n",
        "        self.l1_strength = torch.FloatTensor([l1_strength]).to(device)\n",
        "\n",
        "    def encode(self, x):\n",
        "        e1 = F.relu(self.fc11(x))\n",
        "        e1 = F.relu(self.fc12(e1))\n",
        "        e1 = e1.add(self.fcs(x))\n",
        "        return self.fc21(e1), self.fc22(e1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def generate(self, h):\n",
        "        g1 = torch.tanh(self.fcg1(h))\n",
        "        g1 = torch.tanh(self.fcg2(g1))\n",
        "        g1 = torch.tanh(self.fcg3(g1))\n",
        "        g1 = torch.tanh(self.fcg4(g1))\n",
        "        g1 = g1.add(h)\n",
        "        return g1\n",
        "\n",
        "    def decode(self, z):\n",
        "        d1 = F.softmax(self.fcd1(z), dim=1)\n",
        "        return d1\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        g = self.generate(z)\n",
        "        return z, g, self.decode(g), mu, logvar\n",
        "\n",
        "    def print_topic_words(self, vocab_dic, fn, n_top_words=10):\n",
        "        beta_exp = self.fcd1.weight.data.cpu().numpy().T\n",
        "        \n",
        "        print(\"Writing to %s\" % fn)\n",
        "        fw = open(fn, 'w')\n",
        "        \n",
        "        for k, beta_k in enumerate(beta_exp):\n",
        "            topic_words = [vocab_dic[w_id] for w_id in np.argsort(beta_k)[:-n_top_words - 1:-1]]\n",
        "            \n",
        "            print('Topic {}: {}'.format(k, ' '.join(topic_words)))\n",
        "            \n",
        "            fw.write('{}\\n'.format(' '.join(topic_words)))\n",
        "        fw.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1CpqZKcmu8d",
        "colab_type": "text"
      },
      "source": [
        "### Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnZfqMF-8jD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "def l1_penalty(para):\n",
        "    return nn.L1Loss()(para, torch.zeros_like(para))\n",
        "\n",
        "\n",
        "def check_sparsity(para, sparsity_threshold=1e-3):\n",
        "    num_weights = para.shape[0] * para.shape[1]\n",
        "    num_zero = (para.abs() < sparsity_threshold).sum().float()\n",
        "    return num_zero / float(num_weights)\n",
        "\n",
        "\n",
        "def update_l1(cur_l1, cur_sparsity, sparsity_target):\n",
        "    diff = sparsity_target - cur_sparsity\n",
        "    cur_l1.mul_(2.0 ** diff)\n",
        "\n",
        "def compute_loss(model, dataloader, optimizer, epoch):\n",
        "    \n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch_idx, data_bow in enumerate(dataloader):\n",
        "        data_bow = data_bow.to(device)\n",
        "        \n",
        "        # normalize data\n",
        "        data_bow_norm = F.normalize(data_bow)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        z, g, recon_batch, mu, logvar = model(data_bow_norm)\n",
        "        \n",
        "        loss = loss_function(recon_batch, data_bow, mu, logvar)\n",
        "        loss = loss + model.l1_strength * l1_penalty(model.fcd1.weight)\n",
        "        loss.backward()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    \n",
        "    sparsity = check_sparsity(model.fcd1.weight.data)\n",
        "    print(\"Overall sparsity = %.3f, l1 strength = %.5f\" % (sparsity, model.l1_strength))\n",
        "    print(\"Target sparsity = %.3f\" % target_sparsity)\n",
        "    update_l1(model.l1_strength, sparsity, target_sparsity)\n",
        "    \n",
        "    avg_loss = train_loss / len(dataloader.data)\n",
        "    \n",
        "    print('Train epoch: {} Average loss: {:.4f}'.format(\n",
        "        epoch, avg_loss))\n",
        "    \n",
        "    return sparsity, avg_loss\n",
        "\n",
        "def compute_test_loss(model, dataloader, epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data_bow in enumerate(dataloader):\n",
        "            data_bow = data_bow.to(device)\n",
        "            data_bow_norm = F.normalize(data_bow)\n",
        "\n",
        "            _, _, recon_batch, mu, logvar = model(data_bow_norm)\n",
        "            test_loss += loss_function(recon_batch, data_bow, mu, logvar).item()\n",
        "\n",
        "    avg_loss = test_loss / len(dataloader.data)\n",
        "    print('Test epoch : {} Average loss: {:.4f}'.format(epoch, avg_loss))\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def compute_perplexity(model, dataloader):\n",
        "    \n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, data_bow in enumerate(dataloader):\n",
        "            data_bow = data_bow.to(device)\n",
        "            data_bow_norm = F.normalize(data_bow)\n",
        "            \n",
        "            z, g, recon_batch, mu, logvar = model(data_bow_norm)\n",
        "            \n",
        "            #loss += loss_function(recon_batch, data_bow, mu, logvar).detach()\n",
        "            loss += F.binary_cross_entropy(recon_batch, data_bow, size_average=False)\n",
        "            \n",
        "    loss = loss / dataloader.word_count\n",
        "    perplexity = np.exp(loss.cpu().numpy())\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "\n",
        "def lasy_predict(model, dataloader,vocab_dic, num_example=5, n_top_words=5):\n",
        "    model.eval()\n",
        "    docs, text = dataloader.bow_and_text()\n",
        "    \n",
        "    docs, text = docs[:num_example], text[:num_example]\n",
        "    \n",
        "    docs_device = docs.to(device)\n",
        "    docs_norm = F.normalize(docs_device)\n",
        "    z, _, _, _, _ = model(docs_norm)\n",
        "    z_a = z.detach().cpu().argmax(1).numpy()\n",
        "    z = torch.softmax(z, dim=1).detach().cpu().numpy()\n",
        "    \n",
        "    beta_exp = model.fcd1.weight.data.cpu().numpy().T\n",
        "    topics = []\n",
        "    for k, beta_k in enumerate(beta_exp):\n",
        "        topic_words = [vocab_dic[w_id] for w_id in np.argsort(beta_k)[:-n_top_words - 1:-1]]\n",
        "        topics.append(topic_words)\n",
        "    \n",
        "    for i, (zi, _z_a, t) in enumerate(zip(z, z_a, text)):\n",
        "        \n",
        "        print('\\n===== # {}, Topic : {}, p : {:.4f} %'.format(i+1, _z_a,  zi[_z_a] * 100))\n",
        "        print(\"Topic words :\", ', '.join(topics[_z_a]))\n",
        "        print(\"Input :\", ' '.join(t))\n",
        "        \n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.kaiming_uniform(m.weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQlbYf8tGbEA",
        "colab_type": "text"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM4f1Zz88j7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# set random seeds\n",
        "random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "num_articles = -1\n",
        "\n",
        "# data size limitation\n",
        "max_src_len = 150\n",
        "max_trg_len = 10\n",
        "max_bow_vocab_size=100000\n",
        "\n",
        "# Model parameter\n",
        "hidden_dim = 1000 \n",
        "topic_num = 20\n",
        "target_sparsity=0.85\n",
        "\n",
        "# Training parameter\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "logdir = \"./\"\n",
        "n_epoch = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4brGL2YnL4e",
        "colab_type": "text"
      },
      "source": [
        "### Load Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y9r7XYwnJp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n",
        "stopwords = [line.decode(\"utf-8\").strip() for line in res]\n",
        "res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/English.txt\")\n",
        "stopwords += [line.decode(\"utf-8\").strip() for line in res]\n",
        "\n",
        "stopwords += ['*', '&', '[', ']', ')', '(', '-',':','.','/','0', '...?', '——', '!【', '\"', ')、', ')。', ')」']\n",
        "\n",
        "print(\"# Stopwords : \", len(stopwords))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04TvZdRxm3NM",
        "colab_type": "text"
      },
      "source": [
        "### Load articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FprkMXpWlljc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_path = \"./text/\"\n",
        "doc_dir = Path(doc_path)\n",
        "dirs = [i for i in doc_dir.iterdir() if i.is_dir()]\n",
        "articles = [a for categ in dirs for a in categ.iterdir()]\n",
        "random.shuffle(articles)\n",
        "\n",
        "articles = articles[:num_articles]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrZhMYBHlxMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = docTokenizer(stopwords = stopwords, exclude_reg=r\"\\d(年|月|日)\")\n",
        "\n",
        "docs = []\n",
        "for a in tqdm(articles):\n",
        "    with a.open() as f:\n",
        "        \n",
        "        # discard first two lines.\n",
        "        f.readline()\n",
        "        f.readline()\n",
        "        \n",
        "        docs.append(tokenizer.tokenize(f.read()))\n",
        "        \n",
        "print(len(docs))\n",
        "\n",
        "#bow_vocab = build_bow_vocab(docs, bow_vocab_size = max_bow_vocab_size, stopwords = stopwords)\n",
        "bow_vocab = build_bow_vocab(docs, stopwords = stopwords)\n",
        "bow_vocab_size=len(bow_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrdS-hqP-oWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_valid_size = int(len(docs) * 0.1)\n",
        "\n",
        "test_data  = docs[:test_valid_size]\n",
        "valid_data = docs[test_valid_size : test_valid_size*2]\n",
        "train_data = docs[test_valid_size*2 :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH55xXCcMqVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(valid_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AR_fr0YvBlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgyMYMdCnWQt",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRcwhf0I8mcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorboard\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# building dataloader\n",
        "dataloader = DataLoader(data = train_data, bow_vocab = bow_vocab, batch_size = batch_size)\n",
        "dataloader_valid = DataLoader(data = valid_data, bow_vocab = bow_vocab, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "# builing model and optimiser\n",
        "ntm_model = NTM(input_dim = bow_vocab_size, hidden_dim = hidden_dim, topic_num = topic_num, l1_strength=0.0000001).to(device)\n",
        "optimizer = optim.Adam(ntm_model.parameters(), lr=learning_rate)\n",
        "\n",
        "ntm_model.apply(init_weights)\n",
        "\n",
        "\n",
        "# Start Training\n",
        "for epoch in range(1, n_epoch + 1):\n",
        "    \n",
        "    print(\"======== Epoch\", epoch, \" ========\")\n",
        "    sparsity, train_loss = compute_loss(ntm_model, dataloader, optimizer, epoch)\n",
        "    val_loss = compute_test_loss(ntm_model, dataloader_valid, epoch)\n",
        "    \n",
        "    pp = compute_perplexity(ntm_model, dataloader)\n",
        "    pp_val = compute_perplexity(ntm_model, dataloader_valid)\n",
        "    print(\"PP(train) = %.3f, PP(valid) = %.3f\" % (pp, pp_val))\n",
        "    \n",
        "    writer.add_scalars('scalar/loss',{'train_loss': train_loss,'valid_loss': val_loss},epoch)\n",
        "    writer.add_scalars('scalar/perplexity',{'train_pp': pp,'valid_pp': pp_val},epoch)\n",
        "    writer.add_scalars('scalar/sparsity',{'sparsity': sparsity},epoch)\n",
        "    writer.add_scalars('scalar/l1_strength',{'l1_strength': ntm_model.l1_strength},epoch)\n",
        "    \n",
        "    if epoch % 50 == 0:\n",
        "        ntm_model.print_topic_words(bow_vocab, os.path.join(logdir, 'topwords_e%d.txt' % epoch))\n",
        "        lasy_predict(ntm_model, dataloader_valid, bow_vocab, num_example=10, n_top_words=10)\n",
        "        \n",
        "writer.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IF9WwCPoBb6",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nQx6V4WoA2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader_test  = DataLoader(data = test_data, bow_vocab = bow_vocab, batch_size = batch_size, shuffle=False)\n",
        "pp_test = compute_perplexity(ntm_model, dataloader_test)\n",
        "print(\"PP(test) = %.3f\" % (pp_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtflBZb0oJj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntm_model.print_topic_words(bow_vocab, os.path.join(logdir, 'topwords_e%d.txt' % 9999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UHkbaXfYL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lasy_predict(ntm_model, dataloader_test, bow_vocab, num_example=50, n_top_words=10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}